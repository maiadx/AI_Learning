
We'll reproduce GPT-2 from scratch (124M).

[![Alt text](https://img.youtube.com/vi/Beus3pzegDf7rWHa)]([https://www.youtube.com/Beus3pzegDf7rWHa](https://youtu.be/l8pRSuU81PU?si=Beus3pzegDf7rWHa))

First, let's do a paper reading of "Language Models Are Unsupervised Multitask Learners":
