
We'll reproduce GPT-2 from scratch (124M).

<iframe width="560" height="315" src="https://www.youtube.com/embed/abcdefghijk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

First, let's do a paper reading of "Language Models Are Unsupervised Multitask Learners":
