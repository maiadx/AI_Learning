
We'll reproduce GPT-2 from scratch (124M).

<iframe width="560" height="315" src="https://youtu.be/l8pRSuU81PU?si=Beus3pzegDf7rWHa" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

First, let's do a paper reading of "Language Models Are Unsupervised Multitask Learners":
